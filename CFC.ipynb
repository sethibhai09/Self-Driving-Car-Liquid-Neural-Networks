{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import logging\n",
    "import optuna\n",
    "\n",
    "# Import model-specific modules\n",
    "from ncps.wirings import AutoNCP\n",
    "from ncps.torch import CfC\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Global device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: DrivingDataset Class Definition\n",
    "\n",
    "# Define CSV column names\n",
    "COLUMN_NAMES = [\"center\", \"left\", \"right\", \"steering\", \"throttle\", \"brake\", \"speed\"]\n",
    "\n",
    "class DrivingDataset(Dataset):\n",
    "    \"\"\"Dataset for autonomous driving images and targets.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None, sequence_length=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the CSV file.\n",
    "            root_dir (str): Directory where images are stored.\n",
    "            transform (callable, optional): Transformations to apply to images.\n",
    "            sequence_length (int): Number of consecutive frames per sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file, names=COLUMN_NAMES)\n",
    "        # Keep only required columns\n",
    "        self.df = self.df[[\"center\", \"left\", \"right\", \"steering\", \"throttle\", \"brake\"]]\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def get_image_path(self, col_value):\n",
    "        \"\"\"Construct full image path from CSV entry.\"\"\"\n",
    "        filename = Path(col_value.strip()).name\n",
    "        return self.root_dir / \"IMG\" / filename\n",
    "\n",
    "    def load_image(self, path: Path):\n",
    "        \"\"\"Load image and convert to RGB. On failure, returns a blank image.\"\"\"\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {path}: {e}\")\n",
    "            # Return a blank image (default size 200x66); adjust if necessary.\n",
    "            img = Image.new(\"RGB\", (200, 66))\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of sequences in the dataset.\"\"\"\n",
    "        return len(self.df) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images_seq = []\n",
    "        # Iterate over the sequence of frames\n",
    "        for i in range(self.sequence_length):\n",
    "            row = self.df.iloc[idx + i]\n",
    "            # Construct paths for center, left, and right images\n",
    "            center_path = self.get_image_path(row[\"center\"])\n",
    "            left_path = self.get_image_path(row[\"left\"])\n",
    "            right_path = self.get_image_path(row[\"right\"])\n",
    "            \n",
    "            # Load images with error handling\n",
    "            center_img = self.load_image(center_path)\n",
    "            left_img = self.load_image(left_path)\n",
    "            right_img = self.load_image(right_path)\n",
    "            \n",
    "            # Apply transformations if provided\n",
    "            if self.transform:\n",
    "                center_img = self.transform(center_img)\n",
    "                left_img = self.transform(left_img)\n",
    "                right_img = self.transform(right_img)\n",
    "            \n",
    "            # Stack camera views: resulting shape (3, channels, height, width)\n",
    "            images = torch.stack([center_img, left_img, right_img], dim=0)\n",
    "            images_seq.append(images)\n",
    "        \n",
    "        # Stack sequence: shape (sequence_length, 3, channels, height, width)\n",
    "        images_seq = torch.stack(images_seq, dim=0)\n",
    "        \n",
    "        # Use target values from the last frame\n",
    "        target_row = self.df.iloc[idx + self.sequence_length - 1]\n",
    "        target = torch.tensor([\n",
    "            target_row[\"steering\"],\n",
    "            target_row[\"throttle\"],\n",
    "            target_row[\"brake\"]\n",
    "        ], dtype=torch.float32)\n",
    "        return images_seq, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Cell 3: Data Transformations and Dataset Initialization\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((66, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Update these paths to match your dataset location.\n",
    "csv_path = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\Udacity datset 2\\self_driving_car_dataset_make\\driving_log.csv\"\n",
    "root_dir = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\Udacity datset 2\\self_driving_car_dataset_make\"\n",
    "\n",
    "# Create dataset instance\n",
    "sequence_length = 5\n",
    "dataset = DrivingDataset(csv_file=csv_path, root_dir=root_dir, transform=transform, sequence_length=sequence_length)\n",
    "\n",
    "# Print total samples and inspect one sample\n",
    "print(\"Total samples in dataset:\", len(dataset))\n",
    "sample_images, sample_target = dataset[0]\n",
    "print(\"Sample images shape:\", sample_images.shape)  # Expected: (sequence_length, 3, channels, 66, 200)\n",
    "print(\"Sample target (steering, throttle, brake):\", sample_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: TemporalSequenceLearner Model Definition\n",
    "\n",
    "class TemporalSequenceLearner(nn.Module):\n",
    "    def __init__(self, hidden_neurons, image_channels=3, pretrained_weights_path=None,sparsity_level=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_neurons (int): Number of hidden units for the CfC wiring.\n",
    "            image_channels (int): Number of input image channels (e.g., 3 for RGB).\n",
    "            pretrained_weights_path (str, optional): Path to pretrained weights.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Shared feature extractor for individual images\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 24, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(24, 36, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(36, 48, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(48, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if pretrained_weights_path:\n",
    "            self.feature_extractor.load_state_dict(torch.load(pretrained_weights_path))\n",
    "        # Freeze feature extractor parameters\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Computed feature dimension for images of size (3, 66, 200)\n",
    "        self.feature_dim = 1152\n",
    "        \n",
    "        # Configure AutoNCP to output 3 values (steering, throttle, brake)\n",
    "        wiring = AutoNCP(hidden_neurons, 3,sparsity_level=sparsity_level)\n",
    "        wiring.adjacency_matrix = torch.tensor(wiring.adjacency_matrix).cpu()\n",
    "        if wiring.sensory_adjacency_matrix is not None:\n",
    "            wiring.sensory_adjacency_matrix = torch.tensor(wiring.sensory_adjacency_matrix).cpu()\n",
    "\n",
    "        self.classifier = CfC(self.feature_dim, wiring)\n",
    "        # Final output layer maps CfC output to a 3-element vector.\n",
    "        self.output_layer = nn.Linear(3, 3)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Tensor of shape (batch, T, 3, channels, height, width).\n",
    "        Returns:\n",
    "            Tensor of shape (batch, 3) representing steering, throttle, and brake.\n",
    "        \"\"\"\n",
    "        batch, T, num_views, channels, height, width = images.size()\n",
    "        # Merge batch, time, and view dimensions: shape (batch*T*num_views, channels, height, width)\n",
    "        images = images.view(batch * T * num_views, channels, height, width)\n",
    "        features = self.feature_extractor(images)\n",
    "        features = features.view(features.size(0), -1)  # Flatten features\n",
    "        \n",
    "        # Reshape to (batch, T, num_views, feature_dim) and average over views\n",
    "        features = features.view(batch, T, num_views, self.feature_dim).mean(dim=2)\n",
    "        \n",
    "        # Process temporal sequence with CfC; output shape: (batch, T, 3)\n",
    "        classifier_out, _ = self.classifier(features)\n",
    "        final_time_step = classifier_out[:, -1, :]  # Use last time step\n",
    "        \n",
    "        x = self.output_layer(final_time_step)  # (batch, 3)\n",
    "        # Apply activation constraints: steering with tanh, throttle and brake with sigmoid\n",
    "        steering = torch.tanh(x[:, 0:1])\n",
    "        throttle = torch.sigmoid(x[:, 1:2])\n",
    "        brake = torch.sigmoid(x[:, 2:3])\n",
    "        return torch.cat([steering, throttle, brake], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Data Preparation and Hyperparameter Tuning\n",
    "\n",
    "def prepare_data(sequence_length=5, batch_size=32):\n",
    "    \"\"\"\n",
    "    Prepares training and validation data loaders.\n",
    "    \"\"\"\n",
    "    # Update these paths to match your dataset location.\n",
    "    csv_path = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\Udacity datset 2\\self_driving_car_dataset_make\\driving_log.csv\"\n",
    "    root_dir = r\"C:\\Users\\harsh\\OneDrive\\Desktop\\Udacity datset 2\\self_driving_car_dataset_make\"\n",
    "    jungle_csv_path=r\"C:\\Users\\harsh\\OneDrive\\Desktop\\Udacity datset 2\\self_driving_car_dataset_jungle\\driving_log.csv\"\n",
    "    jungle_root_dir=r\"C:\\Users\\harsh\\OneDrive\\Desktop\\Udacity datset 2\\self_driving_car_dataset_jungle\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((66, 200)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    dataset = DrivingDataset(csv_file=csv_path, root_dir=root_dir, transform=transform, sequence_length=sequence_length)\n",
    "    jungle_dataset=DrivingDataset(csv_file=jungle_csv_path, root_dir=jungle_root_dir, transform=transform, sequence_length=sequence_length)\n",
    "    # Split into Train (70%), CV (20%), Test (10%)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    cv_size = int(0.1 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - cv_size\n",
    "\n",
    "    train_dataset, cv_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, cv_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    cv_loader = DataLoader(cv_dataset, batch_size=32, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    jungle_data_loader=DataLoader(jungle_dataset,batch_size=32,shuffle=False)\n",
    "    return train_loader, cv_loader, test_loader,jungle_data_loader\n",
    "\n",
    "# Prepare data loaders\n",
    "jungle_data_loader,train_loader,cv_loader, test_loader = prepare_data(sequence_length)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter tuning with Optuna.\n",
    "    \"\"\"\n",
    "    hidden_neurons = trial.suggest_int(\"hidden_neurons\", 16, 96, step=4)\n",
    "    sparsity_level = trial.suggest_float(\"sparsity_level\", 0.1, 0.9, step=0.05)\n",
    "    \n",
    "    # Re-prepare data loaders in case of changes.\n",
    "    _, cv_loader, test_loader = prepare_data(sequence_length=5)\n",
    "\n",
    "\n",
    "    model = TemporalSequenceLearner(hidden_neurons,sparsity_level=sparsity_level)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0725)\n",
    "    criterion = nn.MSELoss()\n",
    "    num_epochs = 5  # Short training for tuning\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, target in cv_loader:\n",
    "            images, target = images.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(cv_loader)\n",
    "        trial.report(epoch_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, target in jungle_data_loader:\n",
    "            images, target = images.to(device), target.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# Create an Optuna study and optimize the objective function.\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)  # You can adjust the number of trials\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value (validation loss): {trial.value}\")\n",
    "print(\"  Hyperparameters:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "def final_train(model, train_loader, criterion, optimizer, device, start_epoch, num_epochs):\n",
    "    \"\"\"\n",
    "    Trains the model starting from 'start_epoch' for an additional 'num_epochs'.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, target in train_loader:\n",
    "            images, target = images.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Final Training Epoch [{epoch+1}] Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "def final_eval(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and prints the average loss.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Disable gradient calculation for evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, target in test_loader:\n",
    "            images, target = images.to(device), target.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def main():\n",
    "    # Initialize your model and hyperparameters\n",
    "    final_model = TemporalSequenceLearner(hidden_neurons=36, sparsity_level=0.75)\n",
    "    final_model.to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(final_model.parameters(), lr=0.001)\n",
    "    \n",
    "    checkpoint_path = \"final_temporal_sequence_model.pth\"\n",
    "    start_epoch = 2\n",
    "\n",
    "    # Check if a checkpoint exists and load it\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        final_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Training from scratch.\")\n",
    "\n",
    "    # Specify additional epochs you want to train for\n",
    "    additional_epochs = 0\n",
    "    final_train(final_model, train_loader, criterion, optimizer, device, start_epoch, additional_epochs)\n",
    "    \n",
    "    # Save a new checkpoint with updated epoch count, model state, and optimizer state\n",
    "    total_epochs = start_epoch + additional_epochs\n",
    "    torch.save({\n",
    "        'epoch': total_epochs,\n",
    "        'model_state_dict': final_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Final model saved as '{checkpoint_path}' after {total_epochs} epochs.\")\n",
    "    checkpoint = torch.load(\"final_temporal_sequence_model.pth\", map_location=\"cpu\")\n",
    "    final_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    final_model.eval()\n",
    "    dummy_input = torch.randn(1, 5,3, 3, 66, 200).to(device)  # Batch size=1, Sequence length=5, Channels=3, Height=66, Width=200\n",
    "    torch.onnx.export(final_model, dummy_input, \"model.onnx\")\n",
    "\n",
    "    # # Evaluate the model on the test set\n",
    "    # final_eval(final_model, jungle_data_loader, criterion, device)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 5,3, 3, 66, 200)  # Batch size=1, Sequence length=5, Channels=3, Height=66, Width=200\n",
    "torch.onnx.export(final_model, dummy_input, \"model.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
